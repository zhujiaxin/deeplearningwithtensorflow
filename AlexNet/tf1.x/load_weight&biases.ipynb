{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import AlexNet\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import downloadmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***if download is slow, just go to \"http://www.cs.toronto.edu/~guerzhoy/tf_alexnet\" download file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'bvlc_alexnet.npy'\n",
    "if  not os.path.exists(path):\n",
    "    downloadmodel.download('http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/bvlc_alexnet.npy', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto() \n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4  #占用40%显存\n",
    "sess = tf.Session(config=config)\n",
    "model = AlexNet.AlexNet2()\n",
    "#filewriter = tf.summary.FileWriter('logs/',sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "17472\n",
      "34944\n",
      "188672\n",
      "342400\n",
      "784768\n",
      "1227136\n",
      "1227520\n",
      "1559488\n",
      "1891456\n",
      "2112768\n",
      "2334080\n",
      "40086912\n",
      "56868224\n",
      "60965224\n"
     ]
    }
   ],
   "source": [
    "papameter_all = 0\n",
    "for i in model.parameter:\n",
    "    print(papameter_all)\n",
    "    for tensor_parameter in i:\n",
    "        papameter_all += np.prod(tensor_parameter.get_shape())\n",
    "print(papameter_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "data = np.load(path, allow_pickle=True, fix_imports=True, encoding='latin1')\n",
    "print(len(data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc6', 'fc7', 'fc8']\n"
     ]
    }
   ],
   "source": [
    "data_1 = data.item()\n",
    "keys = sorted(data_1.keys())\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 11, 3, 96)\n",
      "(96,)\n",
      "---\n",
      "(5, 5, 48, 256)\n",
      "(256,)\n",
      "---\n",
      "(3, 3, 256, 384)\n",
      "(384,)\n",
      "---\n",
      "(3, 3, 192, 384)\n",
      "(384,)\n",
      "---\n",
      "(3, 3, 192, 256)\n",
      "(256,)\n",
      "---\n",
      "(9216, 4096)\n",
      "(4096,)\n",
      "---\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "---\n",
      "(4096, 1000)\n",
      "(1000,)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for key in keys:\n",
    "    for i in data_1[key]:\n",
    "        print(np.array(i).shape)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "first_conv_layer_part1/kernel_weight:0\n",
      "(11, 11, 3, 48)\n",
      "first_conv_layer_part1/kernel_bias:0\n",
      "(48,)\n",
      "-------\n",
      "fisrt_conv_layer_part2/kernel_weight:0\n",
      "(11, 11, 3, 48)\n",
      "fisrt_conv_layer_part2/kernel_bias:0\n",
      "(48,)\n",
      "-------\n",
      "second_conv_layer_part1/kernel_weight:0\n",
      "(5, 5, 48, 128)\n",
      "second_conv_layer_part1/kernel_bias:0\n",
      "(128,)\n",
      "-------\n",
      "second_conv_layer_part2/kernel_weight:0\n",
      "(5, 5, 48, 128)\n",
      "second_conv_layer_part2/kernel_bias:0\n",
      "(128,)\n",
      "-------\n",
      "third_conv_layer_part1/kernel_weight1:0\n",
      "(3, 3, 128, 192)\n",
      "third_conv_layer_part1/kernel_weight2:0\n",
      "(3, 3, 128, 192)\n",
      "-------\n",
      "third_conv_layer_part2/kernel_weight1:0\n",
      "(3, 3, 128, 192)\n",
      "third_conv_layer_part2/kernel_weight2:0\n",
      "(3, 3, 128, 192)\n",
      "-------\n",
      "make_two_as_one/bias3_1:0\n",
      "(192,)\n",
      "make_two_as_one/bias3_1_1:0\n",
      "(192,)\n",
      "-------\n",
      "fourth_conv_layer_part1/kernel_weight:0\n",
      "(3, 3, 192, 192)\n",
      "fourth_conv_layer_part1/kernel_bias:0\n",
      "(192,)\n",
      "-------\n",
      "fourth_conv_layer_part2/kernel_weight:0\n",
      "(3, 3, 192, 192)\n",
      "fourth_conv_layer_part2/kernel_bias:0\n",
      "(192,)\n",
      "-------\n",
      "fifth_conv_layer_part1/kernel_weight:0\n",
      "(3, 3, 192, 128)\n",
      "fifth_conv_layer_part1/kernel_bias:0\n",
      "(128,)\n",
      "-------\n",
      "fifth_conv_layer_part2/kernel_weight:0\n",
      "(3, 3, 192, 128)\n",
      "fifth_conv_layer_part2/kernel_bias:0\n",
      "(128,)\n",
      "-------\n",
      "first_fc_layer/fc_weight:0\n",
      "(9216, 4096)\n",
      "first_fc_layer/fc_bias:0\n",
      "(4096,)\n",
      "-------\n",
      "second_fc_layer/fc_weight:0\n",
      "(4096, 4096)\n",
      "second_fc_layer/fc_bias:0\n",
      "(4096,)\n",
      "-------\n",
      "thrid_fc_layer/fc_weight:0\n",
      "(4096, 1000)\n",
      "thrid_fc_layer/fc_bias:0\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "for i in model.parameter:\n",
    "    print('-------')\n",
    "    for j in i:\n",
    "        print(j.name)\n",
    "        print(j.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 11, 3, 48)\n",
      "(11, 11, 3, 48)\n",
      "(48,)\n",
      "(48,)\n"
     ]
    }
   ],
   "source": [
    "value_list= []\n",
    "conv1_weight = data_1['conv1'][0]\n",
    "conv1_bias = data_1['conv1'][1]\n",
    "conv1_weight_1,conv1_weight_2 = np.split(conv1_weight,2,axis=-1)\n",
    "conv1_bias_1,conv1_bias_2 = np.split(conv1_bias,2,axis=-1)\n",
    "print(conv1_weight_1.shape)\n",
    "print(conv1_weight_2.shape)\n",
    "print(conv1_bias_1.shape)\n",
    "print(conv1_bias_2.shape)\n",
    "value_list.append([conv1_weight_1, conv1_bias_1])\n",
    "value_list.append([conv1_weight_2, conv1_bias_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 48, 128)\n",
      "(5, 5, 48, 128)\n",
      "(128,)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "conv2_weight = data_1['conv2'][0]\n",
    "conv2_bias = data_1['conv2'][1]\n",
    "conv2_weight_1,conv2_weight_2 = np.split(conv2_weight,2,axis=-1)\n",
    "conv2_bias_1,conv2_bias_2 = np.split(conv2_bias,2,axis=-1)\n",
    "print(conv2_weight_1.shape)\n",
    "print(conv2_weight_2.shape)\n",
    "print(conv2_bias_1.shape)\n",
    "print(conv2_bias_2.shape)\n",
    "value_list.append([conv2_weight_1, conv2_bias_1])\n",
    "value_list.append([conv2_weight_2, conv2_bias_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 128, 192)\n",
      "(3, 3, 128, 192)\n",
      "(3, 3, 128, 192)\n",
      "(3, 3, 128, 192)\n",
      "(192,)\n",
      "(192,)\n"
     ]
    }
   ],
   "source": [
    "conv3_weight = data_1['conv3'][0]\n",
    "conv3_bias = data_1['conv3'][1]\n",
    "conv3_weight_1,conv3_weight_2 = np.split(conv3_weight,2,axis=2)\n",
    "conv3_weight_1_1,conv3_weight_1_2 = np.split(conv3_weight_1,2,axis=-1)\n",
    "conv3_weight_2_1,conv3_weight_2_2 = np.split(conv3_weight_2,2,axis=-1)\n",
    "conv3_bias_1,conv3_bias_2 = np.split(conv3_bias,2,axis=-1)\n",
    "print(conv3_weight_1_1.shape)\n",
    "print(conv3_weight_1_2.shape)\n",
    "print(conv3_weight_2_1.shape)\n",
    "print(conv3_weight_2_2.shape)\n",
    "print(conv3_bias_1.shape)\n",
    "print(conv3_bias_2.shape)\n",
    "value_list.append([conv3_weight_1_1, conv3_weight_1_2])\n",
    "value_list.append([conv3_weight_2_1, conv3_weight_2_2])\n",
    "value_list.append([conv3_bias_1, conv3_bias_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 192, 192)\n",
      "(3, 3, 192, 192)\n",
      "(192,)\n",
      "(192,)\n"
     ]
    }
   ],
   "source": [
    "conv4_weight = data_1['conv4'][0]\n",
    "conv4_bias = data_1['conv4'][1]\n",
    "conv4_weight_1,conv4_weight_2 = np.split(conv4_weight,2,axis=-1)\n",
    "conv4_bias_1,conv4_bias_2 = np.split(conv4_bias,2,axis=-1)\n",
    "print(conv4_weight_1.shape)\n",
    "print(conv4_weight_2.shape)\n",
    "print(conv4_bias_1.shape)\n",
    "print(conv4_bias_2.shape)\n",
    "value_list.append([conv4_weight_1, conv4_bias_1])\n",
    "value_list.append([conv4_weight_2, conv4_bias_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 192, 128)\n",
      "(3, 3, 192, 128)\n",
      "(128,)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "conv5_weight = data_1['conv5'][0]\n",
    "conv5_bias = data_1['conv5'][1]\n",
    "conv5_weight_1,conv5_weight_2 = np.split(conv5_weight,2,axis=-1)\n",
    "conv5_bias_1,conv5_bias_2 = np.split(conv5_bias,2,axis=-1)\n",
    "print(conv5_weight_1.shape)\n",
    "print(conv5_weight_2.shape)\n",
    "print(conv5_bias_1.shape)\n",
    "print(conv5_bias_2.shape)\n",
    "value_list.append([conv5_weight_1, conv5_bias_1])\n",
    "value_list.append([conv5_weight_2, conv5_bias_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9216, 4096)\n",
      "(4096,)\n"
     ]
    }
   ],
   "source": [
    "fc1_weight = data_1['fc6'][0]\n",
    "fc1_bias = data_1['fc6'][1]\n",
    "print(fc1_weight.shape)\n",
    "print(fc1_bias.shape)\n",
    "value_list.append([fc1_weight, fc1_bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 4096)\n",
      "(4096,)\n"
     ]
    }
   ],
   "source": [
    "fc2_weight = data_1['fc7'][0]\n",
    "fc2_bias = data_1['fc7'][1]\n",
    "print(fc2_weight.shape)\n",
    "print(fc2_bias.shape)\n",
    "value_list.append([fc2_weight, fc2_bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 1000)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "fc3_weight = data_1['fc8'][0]\n",
    "fc3_bias = data_1['fc8'][1]\n",
    "print(fc3_weight.shape)\n",
    "print(fc3_bias.shape)\n",
    "value_list.append([fc3_weight, fc3_bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 11, 3, 48)\n",
      "(48,)\n",
      "(11, 11, 3, 48)\n",
      "(48,)\n",
      "(5, 5, 48, 128)\n",
      "(128,)\n",
      "(5, 5, 48, 128)\n",
      "(128,)\n",
      "(3, 3, 128, 192)\n",
      "(3, 3, 128, 192)\n",
      "(3, 3, 128, 192)\n",
      "(3, 3, 128, 192)\n",
      "(192,)\n",
      "(192,)\n",
      "(3, 3, 192, 192)\n",
      "(192,)\n",
      "(3, 3, 192, 192)\n",
      "(192,)\n",
      "(3, 3, 192, 128)\n",
      "(128,)\n",
      "(3, 3, 192, 128)\n",
      "(128,)\n",
      "(9216, 4096)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096, 1000)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "for i in value_list:\n",
    "    for j in i:\n",
    "        print(j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, value in enumerate(value_list):\n",
    "    for index_1, value_1 in enumerate(value):\n",
    "        sess.run(model.parameter[index][index_1].assign(value_list[index][index_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_1 = data.item()\\nkeys = sorted(data_1.keys())\\nsess= tf.Session()\\nall_parameter = 0 \\nfor index, key in enumerate(keys):\\n    index_ = 0\\n    print(key)\\n    print(all_parameter)\\n    for value in data_1[key]:\\n        #print(model.parameter[index][index_].get_shape())\\n        #print(np.array(value).shape)\\n        \\n        all_parameter += np.prod(np.array(value).shape)\\n        print(type(model.parameter[index][index_]))\\n        sess.run(model.parameter[index][index_].assign(conv1_weight_1))\\n        index_ += 1\\n#print(sess.run(model.parameter[-1][0].eval()))\\nprint(all_parameter)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"data_1 = data.item()\n",
    "keys = sorted(data_1.keys())\n",
    "sess= tf.Session()\n",
    "all_parameter = 0 \n",
    "for index, key in enumerate(keys):\n",
    "    index_ = 0\n",
    "    print(key)\n",
    "    print(all_parameter)\n",
    "    for value in data_1[key]:\n",
    "        #print(model.parameter[index][index_].get_shape())\n",
    "        #print(np.array(value).shape)\n",
    "        \n",
    "        all_parameter += np.prod(np.array(value).shape)\n",
    "        print(type(model.parameter[index][index_]))\n",
    "        sess.run(model.parameter[index][index_].assign(conv1_weight_1))\n",
    "        index_ += 1\n",
    "#print(sess.run(model.parameter[-1][0].eval()))\n",
    "print(all_parameter)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ckpt/my_alexnet'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'ckpt/my_alexnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow1.14",
   "language": "python",
   "name": "tensorflow1.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
